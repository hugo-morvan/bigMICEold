---
title: "SESAR_IV Experiments"
output: html_document
date: Sys.Date()
---

## Libraries import

```{r libraries}
install_if_missing <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    options(repos = c(CRAN = "https://cloud.r-project.org"))
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}
install_if_missing("devtools")
library(devtools)
install_if_missing("mice")
library(mice)
install_if_missing("sparklyr")
library(sparklyr)
install_if_missing("dplyr")
library(dplyr)
install_if_missing("profmem")
library(profmem)
install_if_missing("parallel")
library(parallel)
install_if_missing("dbplot")
library(dbplot)
install_if_missing("ggplot2")
library(ggplot2)

# Force reinstall to make sure latest version is used
install_github("hugo-morvan/bigMice", force = TRUE)
library(bigMice)
```

```{r utils}
# Function to get memory usage in MB for a specific PID
get_memory_usage <- function(pid) {
  status <- readLines(sprintf("/proc/%d/status", pid))
  vmrss <- as.numeric(gsub("[^0-9]", "", 
                           status[grep("VmRSS:", status)]))  # Resident memory size
  return(vmrss / 1024)  # Convert KB to MB
}

monitor_memory <- function(func, ..., 
                           sampling_interval = 0.1,
                           pre_time = 5,    # seconds to monitor before
                           post_time = 10) { # seconds to monitor after
  
  
  # Get the current process ID
  parent_pid <- Sys.getpid()
  
  # Create fixed temporary files
  signal_file <- "/tmp/memory_monitor_signal"
  data_file <- "/tmp/memory_monitor_data.rds"
  start_time_file <- "/tmp/memory_monitor_start.rds"
  
  # Remove any existing signal files
  unlink(c(signal_file, data_file, start_time_file))
  
  # Initialize storage
  memory_data <- data.frame(
    timestamp = numeric(),
    memory_mb = numeric(),
    phase = character()
  )
  
  start_time <- Sys.time()
  
  # Monitor before function call
  #cat("Monitoring pre-execution memory...\n")
  while (difftime(Sys.time(), start_time, units="secs") < pre_time) {
    memory_data <- rbind(memory_data, data.frame(
      timestamp = as.numeric(difftime(Sys.time(), start_time, units="secs")),
      memory_mb = get_memory_usage(parent_pid),
      phase = "pre"
    ))
    Sys.sleep(sampling_interval)
  }
  
  # Save start time and parent PID
  saveRDS(list(start_time = start_time, pid = parent_pid), 
          file = start_time_file)
  
  # Create monitoring script
  monitor_script <- tempfile(fileext = ".R")
  cat('
  get_memory_usage <- function(pid) {
    status <- readLines(sprintf("/proc/%d/status", pid))
    vmrss <- as.numeric(gsub("[^0-9]", "", 
      status[grep("VmRSS:", status)]))
    return(vmrss / 1024)
  }
  
  info <- readRDS("/tmp/memory_monitor_start.rds")
  start_time <- info$start_time
  parent_pid <- info$pid
  signal_file <- "/tmp/memory_monitor_signal"
  data_file <- "/tmp/memory_monitor_data.rds"
  sampling_interval <- ', sampling_interval, '
  
  memory_data <- data.frame(
    timestamp = numeric(),
    memory_mb = numeric(),
    phase = character()
  )
  
  while (!file.exists(signal_file)) {
    current_data <- data.frame(
      timestamp = as.numeric(difftime(Sys.time(), start_time, units="secs")),
      memory_mb = get_memory_usage(parent_pid),
      phase = "during"
    )
    memory_data <- rbind(memory_data, current_data)
    saveRDS(memory_data, file = data_file)
    Sys.sleep(sampling_interval)
  }
  ', file = monitor_script)
  
  # Start monitoring in a separate process
  during_start <- Sys.time()
  cmd <- sprintf("Rscript %s", monitor_script)
  monitoring_pid <- system(cmd, wait = FALSE)
  
  # Execute the main function
  #cat("Executing function and monitoring...\n")
  result <- func(...)
  
  # Signal monitoring to stop and collect data
  file.create(signal_file)
  during_end <- Sys.time()
  
  # Give the monitoring process time to finish and save its last data
  Sys.sleep(1)
  
  # Read and combine the monitoring data
  if (file.exists(data_file)) {
    during_data <- readRDS(data_file)
    memory_data <- rbind(memory_data, during_data)
  }
  
  # Monitor after function completion
  #cat("Monitoring post-execution memory...\n")
  post_start <- Sys.time()
  while (difftime(Sys.time(), post_start, units="secs") < post_time) {
    memory_data <- rbind(memory_data, data.frame(
      timestamp = as.numeric(difftime(Sys.time(), start_time, units="secs")),
      memory_mb = get_memory_usage(parent_pid),
      phase = "post"
    ))
    Sys.sleep(sampling_interval)
  }
  
  # Clean up temporary files
  unlink(c(signal_file, data_file, monitor_script, start_time_file))
  
  # Sort data by timestamp to ensure proper ordering
  memory_data <- memory_data[order(memory_data$timestamp), ]
  
  # Create visualization
  # p <- ggplot(memory_data, aes(x = timestamp, y = memory_mb, color = phase)) +
  #   geom_line(linewidth = 1) +
  #   geom_vline(xintercept = as.numeric(difftime(during_start, start_time, units="secs")),
  #              linetype = "dashed", color = "red") +
  #   geom_vline(xintercept = as.numeric(difftime(during_end, start_time, units="secs")),
  #              linetype = "dashed", color = "red") +
  #   labs(title = "Memory Usage Over Time",
  #        x = "Time (seconds)",
  #        y = "Memory Usage (MB)",
  #        color = "Phase") +
  #   theme_minimal()
  
  #print(p)
  
  # Return both the function result and the memory data
  return(list(
    result = result,
    memory_data = memory_data,
    run_time = as.numeric(difftime(during_end, during_start, units="secs")),
    plot = NULL#p
  ))
}
generate_subset <- function(data, subset_size, replace=FALSE) {
  n = nrow(data)
  sample_size <- round(n * subset_size)
  sampled_indices <- sample(n, sample_size, replace = replace)
  return(data[sampled_indices, ])
}

```

```{r utils2}
create_predictor_matrix <- function(df, save=FALSE){
  # Utility function to create a predictor matrix for bigMICE
  # takes a R dataframe and saves a excel file with the default predictor matrix
  # optional argument save to also save the matrix to an csv file to be modified (easier)

  # Extract variable names
  variables <- colnames(df)

  # Create a TRUE matrix with FALSE on the diagonal
  predictor_matrix <- matrix(TRUE, nrow = length(variables), ncol = length(variables),
                             dimnames = list(variables, variables))
  diag(predictor_matrix) <- FALSE  # Set diagonal to FALSE

  # save the matrix to an excel file
  if (save){
    write.csv(as.data.frame(predictor_matrix), "predictor_matrix.csv")
  }
  return(predictor_matrix)
}

import_predictor_matrix <- function(file){
  # Utility function to import a predictor matrix from an csv file
  # takes a file path and returns the matrix as a matrix object
  data <- read.csv(file, row.names = 1)
  return(as.matrix(data))
}

# Example usage
# path = "C:\\Users\\hugom\\Desktop\\CAMEO\\Code\\sesar_dummy_100.csv"
# df <- read.csv(path)
# predictor_matrix <- create_predictor_matrix(df, save=TRUE)
# numeric_matrix <- ifelse(predictor_matrix, 1, 0)
# heatmap(numeric_matrix, Rowv=NA, Colv=NA, col = c("white", "darkgreen"))

# Modify the predictor matrix in the csv file using excel or sheets
# i use google sheets where the TRUE/FALSE value can be changed using a quick drop down menu.
# It is however a tidious process to decide for each variable if it should be a predictor or not.
# And requires some domain knowledge.
# path_modified = "C:\\Users\\hugom\\Downloads\\predictor_matrix_modified - predictor_matrix.csv"
# predictor_matrix <- import_predictor_matrix(path_modified)
# numeric_matrix <- ifelse(predictor_matrix, 1, 0)
# heatmap(numeric_matrix, Rowv=NA, Colv=NA, col = c("white", "darkgreen"))

```


```{r data_paths}
#path_DORS = "/vault/hugmo418_amed/NDR-SESAR/Uttag Socialstyrelsen 2024/ut_r_par_ov_63851_2023.csv"
path_NDR = "/vault/hugmo418_amed/NDR-SESAR/Uttag SCB+NDR+SESAR 2024/FI_Lev_NDR.csv"
path_SESAR_FU = "/vault/hugmo418_amed/NDR-SESAR/Uttag SCB+NDR+SESAR 2024/FI_Lev_SESAR_FU.csv"
path_SESAR_IV = "/vault/hugmo418_amed/NDR-SESAR/Uttag SCB+NDR+SESAR 2024/FI_Lev_SESAR_IV.csv"
path_SESAR_TS = "/vault/hugmo418_amed/NDR-SESAR/Uttag SCB+NDR+SESAR 2024/FI_Lev_SESAR_TS.csv"
#path_small_SESAR_IV = "/vault/hugmo418_amed/subsets_thesis_hugo/small_IV.csv"

path_info_SESAR_IV = "/home/hugmo418/Documents/bigMICE/Datasets info - Sesar_IV.csv"
path_info_SESAR_FU = "/home/hugmo418/Documents/CAMEO_code/Datasets info - SESAR_FU.csv"
path_info_SESAR_TS ="/home/hugmo418/Documents/CAMEO_code/Datasets info - SESAR_TS.csv"
path_info_NDR = "/home/hugmo418/Documents/CAMEO_code/Datasets info - NDR.csv"
# path_info_DORS = ""

paths = c(path_info_SESAR_IV,path_SESAR_IV,
          path_info_NDR, path_NDR,
          path_info_SESAR_FU, path_SESAR_FU,
          path_info_SESAR_TS, path_SESAR_TS)
# Weird behavior here, file exists, but stopifnot says file dont exists...
# file.exists(path_info_SESAR_IV)
# file.exists(path_SESAR_IV)
#stopifnot(file.exists(paths), "Missing path access to a file")
for(p in paths){
    cat(p)
    if(!file.exists(p)){
        stop(paste0("Missing file", p))
    }else{
        cat("\n found \n")
    }
}
cat("Found all the files.\n")

```

## Read the data

```{r read_data}
data_r <- read.csv(path_SESAR_IV)

```


## Connect to Spark backend

```{r spark_connect}

conf <- spark_config()
conf$`sparklyr.shell.driver-memory`<- "256G"
conf$spark.memory.fraction <- 0.7
conf$`sparklyr.cores.local` <- 32

sc = spark_connect(master = "local", config = conf)

```

## Run the experiment

Create various subset sizes of the dataset.
For each subset size, hide 10% to create known unknowns.
First, run the mice baseline, then the bigMice method.

```{r mice_procedure_def}
mice_procedure <- function(data, m, analysis_formula){
  
  imp <- mice(data, print=F, seed=123, m=m)
  fit <- with(imp, lm(analysis_formula))
  est <- pool(fit)
  
  return(est)
}

results_df <- data.frame(
  sample_size = integer(),      
  repetition = integer(),       
  runtime_sec = numeric(),
  subset_size = numeric(),
  imputation_obj_size = numeric(),
  max_memory_MB = numeric(),    
  RMSE = numeric(),             
  variance = numeric(),         
  predictive_uncertainty = numeric(),
  stringsAsFactors = FALSE
)
```

```{r spark_procedure_def}

spark_procedure <- function(data, m, analysis_formula, variable_types, predictorMatrix){
  
  #imput_init <- bigMice::impute_with_MeMoMe(sc, data, impute_mode = impute_modes)
  
  imputed_results <- bigMice::mice.spark(data = data,
                                           sc = sc,
                               variable_types = variable_types,
                             analysis_formula = analysis_formula,
                              predictorMatrix = predictorMatrix,
                                            m = m,
                                        maxit = 2)
  
  return(imputed_results)
}

```

prep work for the spark procedure:

```{r prep_spark}

get_named_vector <- function(csv_file) {
  # Helper function to extract the named datatypes of a dataset.
  data <- read.csv(csv_file, stringsAsFactors = FALSE)
  named_vector <- setNames(data$Variable.type, data$Variable.Name)
  #names(named_vector)[names(named_vector) == "Age"] <- "IV_Age"
  ordered_vector <- named_vector[order(names(named_vector))] #Order alphabet.
  return(ordered_vector)
}

variable_types <- get_named_vector(path_info_SESAR_IV)
#print(variable_types)
#variable_types <- variable_types[!(names(variable_types) %in% c("LopNr", "SenPNr"))]

#colnames(data_r)


init_dict <- c("Binary" = "mode",
               "Nominal" = "mode",
               "Ordinal" = "mode",
               "Code (don't impute)" = "none", # LopNr, Unit_code etc...
               "Continuous_int" = "median",
               "Continuous_float" = "mean",
               "smalldatetime" = "none",
               "String" = "none", #TBD
               "Count" = "median", #TBD
               "Semi-continuous" = "none", #TBD
               "Else", "none")

impute_modes <- replace(variable_types, variable_types %in% names(init_dict), init_dict[variable_types])

#print(length(variable_types))

preprocess_SESAR_IV <- function(sdf){
    modified_sdf <- sdf
    modified_sdf <- modified_sdf %>%
  #select(-c("LopNr","IV_SenPNr")) %>%
  select(sort(colnames(.))) %>% # Order alphabetically
  mutate(IV_AtrialFibrillation2 = as.numeric(IV_AtrialFibrillation2)) %>%
  mutate(IV_CerebrovascDisease2 = as.numeric(IV_CerebrovascDisease2)) %>%
  mutate(IV_CopdAsthma2 = as.numeric(IV_CopdAsthma2)) %>%
  mutate(IV_CoronaryHeartDisease2 = as.numeric(IV_CoronaryHeartDisease2)) %>%
  mutate(IV_Depression2 = as.numeric(IV_Depression2)) %>%
  mutate(IV_Diabetes2 = as.numeric(IV_Diabetes2)) %>%
  mutate(IV_HeartFailure2 = as.numeric(IV_HeartFailure2)) %>%
  mutate(IV_Hypertension2 = as.numeric(IV_Hypertension2)) %>%
  mutate(IV_TherapyCPAPX = as.numeric(IV_TherapyCPAPX)) %>%
  mutate(IV_TherapyBilevelPAPX = as.numeric(IV_TherapyBilevelPAPX)) %>%
  mutate(IV_TherapySplintX = as.numeric(IV_TherapySplintX)) %>%
  mutate(IV_TherapySurgeryX = as.numeric(IV_TherapySurgeryX))%>%
  mutate(IV_TherapyWeightX = as.numeric(IV_TherapyWeightX))%>%
  mutate(IV_PositionTherapyX = as.numeric(IV_PositionTherapyX))

    return(modified_sdf)
}


```

```{r prep2}
# Prepare the predictor matrix:
predictor_matrix <- create_predictor_matrix(data_r)
#predictor_matrix <- create_predictor_matrix(data_r, save=TRUE) #saves csv to current dir
numeric_matrix <- ifelse(predictor_matrix, 1, 0)
par(bg = "grey")
heatmap(numeric_matrix, Rowv=NA, Colv=NA, 
        col = c("white", "darkgreen"),
        main = "Default Predictor Matrix for SESAR_IV")

path_modified = "~/Documents/CAMEO_code/mod_IV_predictor_matrix.csv"
predictor_matrix_mod <- import_predictor_matrix(path_modified)
numeric_matrix_mod <- t(ifelse(predictor_matrix_mod, 1, 0))
par(bg = "grey")
heatmap(numeric_matrix_mod, Rowv=NA, Colv=NA, 
        col = c("white", "darkgreen"),
        main = "Modified Predictor Matrix for SESAR_IV")

```



Show time:

```{r experiment}
# *** Experiment parameters ***
n_runs = 1
m = 2
subset_sizes = c(0.1, 0.2)#, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
num_subsets <- length(subset_sizes)
# Total iterations = num_subsets * n_runs * m * maxit (5) * 2 (mice and spark)...
# Time per iteration is about 4min for IV (3.8) when using full predictors (default)
# Runtime grows rapidly, need to choose the parameters wisely
analysis_formula <- as.formula("IV_CoronaryHeartDisease2 ~ Age + IV_BMI_Calculated")


for(subset_size in subset_sizes){
    # Create the data subset
    data_subset <- generate_subset(data = data_r, subset_size = subset_size)
    # Create the known unknowns
    #data_res <- generate_known_unknowns(data_subset)
    
    # data_unknowns <- data_res[1]
    # data_knows <- data_res[2]
    data_unknowns <- data_subset
    # print(length(colnames(data_unknowns)))
    # print(length(names(variable_types)))
    # print(setdiff(colnames(data_unknowns), names(variable_types)))
    
    # ********** BASELINE ***********
    
    # ADD COMPLETE CASE ANALYSIS FOR COMPARAISON
    
    # for(i in n_runs){
    #     
    #     monitor_results <- monitor_memory(
    #       mice_procedure,
    #       data = data_unknowns,
    #       sampling_interval = 1,
    #       pre_time = 0,
    #       post_time = 0
    #     )
    #     results_df <- rbind(results_df, data.frame(
    #       sample_size = subset_size, 
    #       repetition = repetition, 
    #       runtime_sec = monitor_results$run_time,
    #       subset_size = subset_size,
    #       imputation_obj_size = monitor_results$result$imp_size,
    #       max_memory_MB = monitor_results$memory_data$memory_mb %>% max(),
    #       stats = list(monitor_results$result)
    #     ))
    #     
    # }
    
    
    # *********** SPARK *************
    # Create the spark dataframe
    sdf <- sdf_copy_to(sc, data_unknowns, overwrite = TRUE)
    sdf <- preprocess_SESAR_IV(sdf)
    for(i in 1:n_runs){
        cat("\n---Run:", i,"---\n")
        start_time <- Sys.time()
        res <- spark_procedure(sdf, m, analysis_formula, variable_types,
                               predictorMatrix = predictor_matrix_mod)
        
        end_time <- Sys.time()
        runtime <- as.numeric(difftime(end_time, start_time, units="secs"))
        cat("\nRuntime:", runtime)
        # How to check the known unknowns for the spark method ? it does not return the data...
        # maybe return the last imputation ? 
    }
    
    
    
    
}
```


```{r}
res
```

res
$rubin_stats
$rubin_stats$`(Intercept)`
$rubin_stats$`(Intercept)`$pooled_param
[1] -8.436439

$rubin_stats$`(Intercept)`$within_var
[1] 0

$rubin_stats$`(Intercept)`$between_var
[1] 0

$rubin_stats$`(Intercept)`$total_var
[1] 0

$rubin_stats$`(Intercept)`$values
[1] -8.436439 -8.436439


$rubin_stats$Age
$rubin_stats$Age$pooled_param
[1] 0.07542945

$rubin_stats$Age$within_var
[1] 0

$rubin_stats$Age$between_var
[1] 0

$rubin_stats$Age$total_var
[1] 0

$rubin_stats$Age$values
[1] 0.07542945 0.07542945


$rubin_stats$IV_BMI_Calculated
$rubin_stats$IV_BMI_Calculated$pooled_param
[1] 0.01510579

$rubin_stats$IV_BMI_Calculated$within_var
[1] 8.501175e-35

$rubin_stats$IV_BMI_Calculated$between_var
[1] 3.40047e-34

$rubin_stats$IV_BMI_Calculated$total_var
[1] 5.950823e-34

$rubin_stats$IV_BMI_Calculated$values
[1] 0.01510579 0.01510579



$per_imputation

$imputation_stats
$imputation_stats[[1]]
$imputation_stats[[1]]$imputation_number
[1] 1

$imputation_stats[[1]]$imputation_time
elapsed 
648.955 

$imputation_stats[[1]]$`(Intercept)`
[1] -8.436439

$imputation_stats[[1]]$Age
[1] 0.07542945

$imputation_stats[[1]]$IV_BMI_Calculated
[1] 0.01510579


$imputation_stats[[2]]
$imputation_stats[[2]]$imputation_number
[1] 2

$imputation_stats[[2]]$imputation_time
elapsed 
646.673 

$imputation_stats[[2]]$`(Intercept)`
[1] -8.436439

$imputation_stats[[2]]$Age
[1] 0.07542945

$imputation_stats[[2]]$IV_BMI_Calculated
[1] 0.01510579



$model_params
$model_params[[1]]
      (Intercept)               Age IV_BMI_Calculated 
      -8.43643879        0.07542945        0.01510579 

$model_params[[2]]
      (Intercept)               Age IV_BMI_Calculated 
      -8.43643879        0.07542945        0.01510579 


## Visualization of the results (needs works)

```{r viz1}
runtime_summary <- results_df %>%
  group_by(sample_size) %>%
  summarize(
    mean_runtime = mean(runtime_sec),
    variance_runtime = var(runtime_sec),  # Variance of runtime across n_runs repetitions
    .groups = "drop"
  )
n <- nrow(data)
ggplot(runtime_summary, aes(x = sample_size*n, y = mean_runtime)) +
  geom_point(color = "blue", size = 3) +    # Mean runtime points
  geom_errorbar(aes(ymin = mean_runtime - sqrt(variance_runtime), 
                    ymax = mean_runtime + sqrt(variance_runtime)), 
                width = 50, color = "red") + # Error bars using standard deviation
  geom_line(color = "black") +              # Line to connect points
  labs(title = "Mean Runtime vs Sample Size",
       x = "Sample Size",
       y = "Mean Runtime (seconds)") +
  theme_minimal()
```

```{r viz2}
imp_size_summary <- results_df %>%
  group_by(sample_size) %>%
  summarize(
    mean_imputation_size = mean(imputation_obj_size),
    variance_imputation_size = var(imputation_obj_size),
    .groups = "drop"
  )
n <- nrow(data)
ggplot(imp_size_summary, aes(x = sample_size*n, y = mean_imputation_size)) +
  geom_point(color = "blue", size = 3) +
  geom_line(color = "black") +
  labs(title = "Mean Imputation size vs Sample Size (m=2)",
       x = "Sample Size",
       y = "Mean Imputation size (MB)") +
  theme_minimal()
```

```{r viz3}
max_mem_summary <- results_df %>%
  group_by(sample_size) %>%
  summarize(
    max_memory_usage = max(max_memory_MB),
    .groups = "drop"
  )
n <- nrow(data)
ggplot(max_mem_summary, aes(x = sample_size*n, y = max_memory_usage)) +
  geom_point(color = "blue", size = 3) +
  geom_line(color = "black") +
  labs(title = "Max memory Usage (MB) during imputation procedure (m=2) vs Sample Size ",
       x = "Sample Size",
       y = "Max memory usage (MB)") +
  theme_minimal()
```

## Disconnect Spark

```{r spark_disconnect}
spark_disconnect(sc)
```

